{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize:\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(Y, R, YMean):\n",
    "        m, n = Y.shape\n",
    "        \n",
    "        if YMean is None:\n",
    "            YMeanVals = np.zeros((m, 1))\n",
    "            \n",
    "        YNorm = np.zeros(Y.shape)\n",
    "\n",
    "        for i in range(0, m):\n",
    "            idx = np.where(R[i, :] == 1)\n",
    "            if YMean is None:\n",
    "                YMeanVals[i] = np.mean(Y[i, idx])\n",
    "                YNorm[i, idx] = Y[i, idx] - YMeanVals[i]\n",
    "            else:\n",
    "                YNorm[i, idx] = Y[i, idx] - YMean[i]\n",
    "            \n",
    "        return YNorm, YMean if YMean is not None else YMeanVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFilter():\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.Y, \\\n",
    "        self.R, \\\n",
    "        self.numUser, \\\n",
    "        self.numMovies, \\\n",
    "        self.numFeature, \\\n",
    "        self.lam, \\\n",
    "        self.batchSize, \\\n",
    "        self.learner,\\\n",
    "        self.anim, \\\n",
    "        self.costs, \\\n",
    "        self.counts = args\n",
    "        \n",
    "    \n",
    "    def cost(self, params):\n",
    "        # Args\n",
    "        #learning_rate, itr\n",
    "        X = np.reshape(params[0:self.numMovies * self.numFeature], (self.numMovies, self.numFeature))\n",
    "        Theta = np.reshape(params[self.numMovies * self.numFeature:], (self.numUser, self.numFeature))\n",
    "\n",
    "        J = sum(sum(self.R * np.square((Theta.dot(X.T)).T - self.Y))) / 2 + \\\n",
    "                (self.lam * sum(sum(np.square(Theta)))) / 2 + \\\n",
    "                (self.lam * sum(sum(np.square(X)))) / 2\n",
    "        \n",
    "        self.costs.append(np.divide(J, 1000))\n",
    "        self.counts.append(len(self.costs))\n",
    "        \n",
    "        if self.anim is not None:\n",
    "            self.anim.plotScores(np.array(self.counts), np.array(self.costs))\n",
    "        \n",
    "        return J\n",
    "\n",
    "    def gradient(self, params):\n",
    "        # Args\n",
    "        #learning_rate, itr\n",
    "        X = np.reshape(params[0:self.numMovies * self.numFeature], (self.numMovies, self.numFeature))\n",
    "        Theta = np.reshape(params[self.numMovies * self.numFeature:], (self.numUser, self.numFeature))\n",
    "\n",
    "        grad = np.multiply((np.dot(X, Theta.T) - self.Y), self.R)\n",
    "        # gradients\n",
    "        if self.learner is 'movie':\n",
    "            X_grad = np.dot(grad, Theta) + self.lam * X\n",
    "            Theta_grad = Theta\n",
    "        elif self.learner is 'user':\n",
    "            X_grad = X\n",
    "            Theta_grad = np.dot(grad.T, X) + self.lam * Theta\n",
    "        else:\n",
    "            X_grad = np.dot(grad, Theta) + self.lam * X\n",
    "            Theta_grad = np.dot(grad.T, X) + self.lam * Theta\n",
    "\n",
    "        params = np.concatenate((X_grad, Theta_grad)).ravel()\n",
    "\n",
    "        return params\n",
    "    \n",
    "    def stochasticGradient(self, params):\n",
    "        #learning_rate, itr\n",
    "        X = np.reshape(params[0:self.numMovies * self.numFeature], (self.numMovies, self.numFeature))\n",
    "        Theta = np.reshape(params[self.numMovies * self.numFeature:], (self.numUser, self.numFeature))\n",
    "        \n",
    "        # There is eps option to pass the learning rate but somehow\n",
    "        # I don't trust that function.\n",
    "#         itr.append(1)\n",
    "#         lr = (1. / (1. + 0.01 * len(itr)))\n",
    "        \n",
    "        if self.learner is 'movie':\n",
    "            X_grad = self.stochasticGradientMovie(X, Theta)\n",
    "            Theta_grad = Theta_grad\n",
    "        elif self.learner is 'user':\n",
    "            X_grad = X\n",
    "            Theta_grad = self.stochasticGradientUser(X, Theta)\n",
    "        else:\n",
    "            X_grad = self.stochasticGradientMovie( X, Theta)\n",
    "            Theta_grad = self.stochasticGradientUser( X, Theta)\n",
    "\n",
    "        params = np.concatenate((X_grad, Theta_grad)).ravel()\n",
    "\n",
    "        return params\n",
    "    \n",
    "    def stochasticGradientMovie(self, X, Theta):\n",
    "        \n",
    "        # (1628 * 10)\n",
    "        X_grad = np.zeros(X.shape)\n",
    "        \n",
    "        # This is stochastic gradient implementation, k is number of features\n",
    "        # for i in n_m:\n",
    "        #     for j in n_u:\n",
    "        #        x_i_k = ((x_i_k * theta_j_k) - y_i_j) * theta_j_k + lam * x_i_k\n",
    "        # Since we are calculating n_m gradients for n_m movies separately, there is no point\n",
    "        # looping over number of movies. And so vectorizing the x_i_k gives below implementation.\n",
    "        # So, to summarize (we are calculating the gradient of each movie for one user at a time)\n",
    "        for i in range(0, self.numUser, self.batchSize):\n",
    "\n",
    "            ThetaTemp = Theta[i:i+batchSize,:]\n",
    "            grad = np.multiply((np.dot(X, ThetaTemp.T) - self.Y[:, i:i+self.batchSize]), self.R[:, i:i+self.batchSize])\n",
    "            #lr * \n",
    "            X_grad = np.dot(grad, ThetaTemp) + self.lam * X\n",
    "            \n",
    "        return X_grad\n",
    "    \n",
    "    def stochasticGradientUser(self):\n",
    "        # (944 * 10)\n",
    "        Theta_grad = np.zeros(Theta.shape)\n",
    "        \n",
    "        # we are calculating the gradient of each user for one movie at a time\n",
    "        for i in range(0, self.numMovies, self.batchSize):\n",
    "\n",
    "            XTemp = X[i:i+self.batchSize,:]\n",
    "            grad = np.multiply((np.dot(XTemp, Theta.T) - self.Y[i:i+self.batchSize, :]), self.R[i:i+self.batchSize, :])\n",
    "            #lr * \n",
    "            Theta_grad = np.dot(grad.T, XTemp) + self.lam * Theta\n",
    "            \n",
    "        return Theta_grad\n",
    "    \n",
    "    def batchGradient(self, params):\n",
    "        #learning_rate, itr\n",
    "        X = np.reshape(params[0:self.numMovies * self.numFeature], (self.numMovies, self.numFeature))\n",
    "        Theta = np.reshape(params[self.numMovies * self.numFeature:], (self.numUser, self.numFeature))\n",
    "        \n",
    "        # There is eps option to pass the learning rate but somehow\n",
    "        # I don't trust that function.\n",
    "#         itr.append(1)\n",
    "#         lr = (1. / (1. + 0.01 * len(itr)))\n",
    "\n",
    "        X_grad = np.zeros(X.shape)\n",
    "        Theta_grad = np.zeros(Theta.shape)\n",
    "        \n",
    "        for i in range(0, self.numMovies, self.batchSize):\n",
    "            idx = np.where(self.R[i:i+self.batchSize, :] == 1)\n",
    "            ThetaTemp = Theta[idx[1]]\n",
    "            YTemp = self.Y[i:i+batchSize, idx[1]]\n",
    "            \n",
    "            #lr * \n",
    "            X_grad[i:i+self.batchSize, :] = ((np.dot(X[i:i+self.batchSize, :], ThetaTemp.T) - YTemp).dot(ThetaTemp) \\\n",
    "                                                + self.lam * X[i:i+self.batchSize, :])\n",
    "\n",
    "        for i in range(0, self.numUser, self.batchSize):\n",
    "            idx = np.where(self.R[:, i:i+self.batchSize] == 1)\n",
    "            XTemp = X[idx[0]]\n",
    "            YTemp = self.Y[idx[0], i:i+self.batchSize]\n",
    "            \n",
    "            #lr * \n",
    "            Theta_grad[i:i+self.batchSize, :] = ((np.dot(XTemp,Theta[i:i+self.batchSize, :].T) - YTemp).T.dot(XTemp) \\\n",
    "                                                + self.lam * Theta[i:i+self.batchSize, :])\n",
    "\n",
    "        params = np.concatenate((X_grad, Theta_grad)).ravel()\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CollaborativeFilter:\n",
    "\n",
    "#     @staticmethod\n",
    "#     def cost(params, *args):\n",
    "#         # Args\n",
    "#         #learning_rate, itr\n",
    "#         Y, R, numUser, numMovies, numFeature, lam, batchSize, learner,anim, costs, counts = args\n",
    "#         X = np.reshape(params[0:numMovies * numFeature], (numMovies, numFeature))\n",
    "#         Theta = np.reshape(params[numMovies * numFeature:], (numUser, numFeature))\n",
    "\n",
    "#         J = sum(sum(R * np.square((Theta.dot(X.T)).T - Y))) / 2 + (lam * sum(sum(np.square(Theta)))) / 2 + (\n",
    "#                 lam * sum(sum(np.square(X)))) / 2\n",
    "        \n",
    "#         costs.append(np.divide(J, 1000))\n",
    "#         counts.append(len(costs))\n",
    "        \n",
    "#         if anim is not None:\n",
    "#             anim.plotScores(np.array(counts), np.array(costs))\n",
    "        \n",
    "#         return J\n",
    "\n",
    "#     @staticmethod\n",
    "#     def gradient(params, *args):\n",
    "#         # Args\n",
    "#         #learning_rate, itr\n",
    "#         Y, R, numUser, numMovies, numFeature, lam, batchSize,learner, anim, costs, counts = args\n",
    "#         X = np.reshape(params[0:numMovies * numFeature], (numMovies, numFeature))\n",
    "#         Theta = np.reshape(params[numMovies * numFeature:], (numUser, numFeature))\n",
    "\n",
    "#         grad = np.multiply((np.dot(X, Theta.T) - Y), R)\n",
    "#         # gradients\n",
    "#         if learner is 'movie':\n",
    "#             X_grad = np.dot(grad, Theta) + lam * X\n",
    "#             Theta_grad = Theta\n",
    "#         elif learner is 'user':\n",
    "#             X_grad = X\n",
    "#             Theta_grad = np.dot(grad.T, X) + lam * Theta\n",
    "#         else:\n",
    "#             X_grad = np.dot(grad, Theta) + lam * X\n",
    "#             Theta_grad = np.dot(grad.T, X) + lam * Theta\n",
    "\n",
    "#         params = np.concatenate((X_grad, Theta_grad)).ravel()\n",
    "\n",
    "#         return params\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def stochasticGradient(params, *args):\n",
    "#         #learning_rate, itr\n",
    "#         Y, R, numUser, numMovies, numFeature, lam, batchSize, learner, anim, costs, counts = args\n",
    "#         X = np.reshape(params[0:numMovies * numFeature], (numMovies, numFeature))\n",
    "#         Theta = np.reshape(params[numMovies * numFeature:], (numUser, numFeature))\n",
    "        \n",
    "#         # There is eps option to pass the learning rate but somehow\n",
    "#         # I don't trust that function.\n",
    "# #         itr.append(1)\n",
    "# #         lr = (1. / (1. + 0.01 * len(itr)))\n",
    "        \n",
    "#         if learner is 'movie':\n",
    "#             X_grad = \\\n",
    "#                 CollaborativeFilter.stochasticGradientMovie(Y, R, numUser, numMovies, numFeature, lam, batchSize, X, Theta)\n",
    "#             Theta_grad = Theta_grad\n",
    "#         elif learner is 'user':\n",
    "#             X_grad = X\n",
    "#             Theta_grad = \\\n",
    "#                 CollaborativeFilter.stochasticGradientUser(Y, R, numUser, numMovies, numFeature, lam, batchSize, X, Theta)\n",
    "#         else:\n",
    "#             X_grad = \\\n",
    "#                 CollaborativeFilter.stochasticGradientMovie(Y, R, numUser, numMovies, numFeature, lam, batchSize, X, Theta)\n",
    "#             Theta_grad = \\\n",
    "#                 CollaborativeFilter.stochasticGradientUser(Y, R, numUser, numMovies, numFeature, lam, batchSize, X, Theta)\n",
    "\n",
    "#         params = np.concatenate((X_grad, Theta_grad)).ravel()\n",
    "\n",
    "#         return params\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def stochasticGradientMovie(Y, R, numUser, numMovies, numFeature, lam, batchSize, X, Theta):\n",
    "        \n",
    "#         # (1628 * 10)\n",
    "#         X_grad = np.zeros(X.shape)\n",
    "        \n",
    "#         # This is stochastic gradient implementation, k is number of features\n",
    "#         # for i in n_m:\n",
    "#         #     for j in n_u:\n",
    "#         #        x_i_k = ((x_i_k * theta_j_k) - y_i_j) * theta_j_k + lam * x_i_k\n",
    "#         # Since we are calculating n_m gradients for n_m movies separately, there is no point\n",
    "#         # looping over number of movies. And so vectorizing the x_i_k gives below implementation.\n",
    "#         # So, to summarize (we are calculating the gradient of each movie for one user at a time)\n",
    "#         for i in range(0, numUser, batchSize):\n",
    "\n",
    "#             ThetaTemp = Theta[i:i+batchSize,:]\n",
    "#             grad = np.multiply((np.dot(X, ThetaTemp.T) - Y[:, i:i+batchSize]), R[:, i:i+batchSize])\n",
    "#             #lr * \n",
    "#             X_grad = np.dot(grad, ThetaTemp) + lam * X\n",
    "            \n",
    "#         return X_grad\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def stochasticGradientUser(Y, R, numUser, numMovies, numFeature, lam, batchSize, X, Theta):\n",
    "#         # (944 * 10)\n",
    "#         Theta_grad = np.zeros(Theta.shape)\n",
    "        \n",
    "#         # we are calculating the gradient of each user for one movie at a time\n",
    "#         for i in range(0, numMovies, batchSize):\n",
    "\n",
    "#             XTemp = X[i:i+batchSize,:]\n",
    "#             grad = np.multiply((np.dot(XTemp, Theta.T) - Y[i:i+batchSize, :]), R[i:i+batchSize, :])\n",
    "#             #lr * \n",
    "#             Theta_grad = np.dot(grad.T, XTemp) + lam * Theta\n",
    "            \n",
    "#         return Theta_grad\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def batchGradient(params, *args):\n",
    "#         #learning_rate, itr\n",
    "#         Y, R, numUser, numMovies, numFeature, lam, batchSize, anim, costs, counts = args\n",
    "#         X = np.reshape(params[0:numMovies * numFeature], (numMovies, numFeature))\n",
    "#         Theta = np.reshape(params[numMovies * numFeature:], (numUser, numFeature))\n",
    "        \n",
    "#         # There is eps option to pass the learning rate but somehow\n",
    "#         # I don't trust that function.\n",
    "# #         itr.append(1)\n",
    "# #         lr = (1. / (1. + 0.01 * len(itr)))\n",
    "\n",
    "#         X_grad = np.zeros(X.shape)\n",
    "#         Theta_grad = np.zeros(Theta.shape)\n",
    "        \n",
    "#         for i in range(0, numMovies, batchSize):\n",
    "#             idx = np.where(R[i:i+batchSize, :] == 1)\n",
    "#             ThetaTemp = Theta[idx[1]]\n",
    "#             YTemp = Y[i:i+batchSize, idx[1]]\n",
    "            \n",
    "#             #lr * \n",
    "#             X_grad[i:i+batchSize, :] = ((np.dot(X[i:i+batchSize, :], ThetaTemp.T) - YTemp).dot(ThetaTemp) \\\n",
    "#                                                 + lam * X[i:i+batchSize, :])\n",
    "\n",
    "#         for i in range(0, numUser, batchSize):\n",
    "#             idx = np.where(R[:, i:i+batchSize] == 1)\n",
    "#             XTemp = X[idx[0]]\n",
    "#             YTemp = Y[idx[0], i:i+batchSize]\n",
    "            \n",
    "#             print(\"Shapes: \", X.shape , XTemp.shape, YTemp.shape, Theta_grad.shape, Theta_grad[i:i+batchSize, :].shape)\n",
    "#             #lr * \n",
    "#             Theta_grad[i:i+batchSize, :] = ((np.dot(XTemp,Theta[i:i+batchSize, :].T) - YTemp).T.dot(XTemp) \\\n",
    "#                                                 + lam * Theta[i:i+batchSize, :])\n",
    "\n",
    "#         params = np.concatenate((X_grad, Theta_grad)).ravel()\n",
    "\n",
    "#         return params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
